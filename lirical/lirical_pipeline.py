import json
import hail as hl
import os
import re

import pandas as pd
from step_pipeline import pipeline, Backend, Localize, Delocalize

DOCKER_IMAGE = "weisburd/lirical@sha256:8f056f67153e4d873c27508fb9effda9c8fa0a1f2dc87777a58266fed4f8c82b"


def define_args(pipeline):
    """Define command-line args for the LIRICAL pipeline.

    Args:
        pipeline (step_pipeline._Pipeline): The step_pipeline pipeline object.
    """
    parser = pipeline.get_config_arg_parser()
    grp = parser.add_argument_group("LIRICAL")
    grp.add_argument("-d", "--lirical-data-dir",
                     help="Google Storage path of the LIRICAL reference data directory generated by the LIRICAL "
                          "download command",
                     default="gs://lirical-reference-data/LIRICAL/data")
    grp.add_argument("-e", "--exomiser-data-dir",
                     help="Google Storage path of the Exomiser reference data directory required by LIRICAL. "
                          "For example 'gs://reference-data-bucket/exomiser-cli-13.0.0/2109_hg38'",
                     default="gs://lirical-reference-data/exomiser-cli-13.0.0/2109_hg38")
    grp.add_argument("-o", "--output-dir",
                     help="Google Storage directory where to write LIRICAL output",
                     required=True)

    thresholds_grp = grp.add_mutually_exclusive_group()
    thresholds_grp.add_argument("-m", "--mindiff",
                                help="Minimum number of differential diagnoses to show in the HTML output, regardless "
                                     "of their post-test probability",
                                type=int)
    thresholds_grp.add_argument("-t", "--threshold",
                                help="Post-test probability threshold as a percentage. Diagnoses with a post-test "
                                     "probability above this threshold will be included in the HTML output.",
                                type=float)

    grp.add_argument("--transcriptdb",
                     help="Which transcript models to use",
                     choices={"UCSC", "Ensembl", "RefSeq"})
    grp.add_argument("--orphanet",
                     help="Use annotation data from Orphanet",
                     action="store_true")
    grp.add_argument("-g", "--use-global",
                     help="Run LIRICAL with the --global flag",
                     action="store_true")

    grp.add_argument("--vcf",
                     help="Google Storage path that contains single-sample VCFs referenced by the phenopackets. More "
                          "than one path can be provided by specifying this argument more than once. Also each path "
                          "can optionally contain wildcards (*).",
                     required=True,
                     action="append")
    grp.add_argument("phenopacket_paths",
                     nargs="+",
                     help="Google Storage path of Phenopacket JSON files to process. More than one path can be "
                          "specified. Also each path can optionally contain wildcards (*).")

    grp.add_argument("-s", "--sample-id", help="Optionally, process only this sample id. Useful for testing.")


def parse_args(pipeline):
    """Define and parse command-line args.

    Args:
        pipeline (step_pipeline._Pipeline): The step_pipeline pipeline object.

    Return:
         argparse.Namespace: parsed command-line args
         pandas.DataFrame: DataFrame with 1 row per phenopacket and columns: "sample_id", "phenopacket_path", "vcf_path"
    """

    define_args(pipeline)
    args = pipeline.parse_args()

    parser = pipeline.get_config_arg_parser()

    # initialize hail with workaround for Hadoop bug involving requester-pays buckets:
    # https://discuss.hail.is/t/im-encountering-bucket-is-a-requester-pays-bucket-but-no-user-project-provided/2536/2
    def get_bucket(path):
        if not path.startswith("gs://"):
            parser.error(f"{path} must start with gs://")
        return re.sub("^gs://", "", path).split("/")[0]

    all_buckets = {
        get_bucket(path) for path in [args.lirical_data_dir, args.exomiser_data_dir] + args.phenopacket_paths + args.vcf
    }

    hl.init(log="/dev/null", quiet=True, idempotent=True, spark_conf={
        "spark.hadoop.fs.gs.requester.pays.mode": "CUSTOM",
        "spark.hadoop.fs.gs.requester.pays.buckets": ",".join(all_buckets),
        "spark.hadoop.fs.gs.requester.pays.project.id": args.gcloud_project,
    })

    # validate input paths
    def check_paths(paths):
        checked_paths = []
        for path in paths:
            if not path.startswith("gs://"):
                parser.error(f"Path must start with gs:// {path}")
            current_paths = [r["path"] for r in hl.hadoop_ls(path)]
            if not current_paths:
                parser.error(f"{path} not found")
            checked_paths += current_paths
        return checked_paths

    phenopacket_paths = check_paths(args.phenopacket_paths)
    vcf_paths = check_paths(args.vcf)

    # create DataFrame of phenopackets to process, with columns: "sample_id", "phenopacket_path", "vcf_path"
    rows = []
    requested_sample_id_found = False
    print(f"Processing {len(phenopacket_paths)} phenopacket(s)")
    for phenopacket_path in phenopacket_paths:
        print(f"Parsing {phenopacket_path}")
        with hl.hadoop_open(phenopacket_path, "r") as f:
            phenopacket_json = json.load(f)
            sample_id = phenopacket_json.get("subject", {}).get("id")
            if args.sample_id:
                if args.sample_id != sample_id:
                    continue
                else:
                    requested_sample_id_found = True

            if sample_id is None:
                parser.error(f"{phenopacket_path} is missing a 'subject' section")

            if ("htsFiles" not in phenopacket_json or not isinstance(phenopacket_json["htsFiles"], list) or
                    "uri" not in phenopacket_json["htsFiles"][0]):
                parser.error(f"{phenopacket_path} is missing an 'htsFiles' section with a VCF uri")
            vcf_filename = phenopacket_json["htsFiles"][0]["uri"].replace("file:///", "")

            matching_vcf_paths = [vcf_path for vcf_path in vcf_paths if vcf_filename in vcf_path]
            if not matching_vcf_paths:
                parser.error(f"Couldn't find {vcf_filename} referred to by {phenopacket_path}")
            vcf_path = matching_vcf_paths[0]

        rows.append({
            "sample_id": sample_id,
            "phenopacket_path": phenopacket_path,
            "vcf_path": vcf_path,
        })
        if requested_sample_id_found:
            break

    metadata_df = pd.DataFrame(rows)

    return args, metadata_df


def main():
    bp = pipeline("LIRICAL", backend=Backend.HAIL_BATCH_SERVICE, config_file_path="~/.step_pipeline")
    args, metadata_df = parse_args(bp)

    for _, row in metadata_df.iterrows():
        s1 = bp.new_step(f"LIRICAL: {row.sample_id}", image=DOCKER_IMAGE, cpu=2, storage="70Gi", memory="highmem",
                         localize_by=Localize.GSUTIL_COPY, delocalize_by=Delocalize.COPY)

        s1.switch_gcloud_auth_to_user_account()
        phenopacket_input = s1.input(row.phenopacket_path)
        vcf_input = s1.input(row.vcf_path)
        lirical_data_dir_input = s1.input(args.lirical_data_dir)
        exomiser_data_dir_input = s1.input(args.exomiser_data_dir)

        s1.command("cd /io/")
        s1.command("set -ex")
        s1.command(f"ln -s {lirical_data_dir_input} data")

        # the vcf's path within the container needs to match the vcf path specified in the phenopacket
        if row.vcf_path.endswith("gz"):
            unzipped_vcf_path = re.sub("(.bgz|.gz)$", "", os.path.basename(vcf_input.local_path))
            # filter out ":NA:" fields to work around a bug where DP="NA" in some VCF rows.
            s1.command(f"gunzip -c {vcf_input} | grep -v :NA: > /{unzipped_vcf_path}")
        else:
            s1.command(f"ln -s {vcf_input} /{vcf_input.filename}")

        lirical_command = f"java -jar /LIRICAL.jar P -p {phenopacket_input} -e {exomiser_data_dir_input} --tsv"
        if args.use_global:
            lirical_command += " --global"
        if args.orphanet:
            lirical_command += " --orphanet"
        if args.threshold is not None:
            lirical_command += f" --threshold {args.threshold}"
        if args.mindiff is not None:
            lirical_command += f" --mindiff {args.mindiff}"
        if args.transcriptdb:
            lirical_command += f" --transcriptdb {args.transcriptdb}"
        s1.command(lirical_command)

        #output_path_prefix = os.path.join(args.output_dir, f"{row.sample_id}.lirical")
        phenopacket_input_prefix = re.sub("(.phenopacket)?.json$", "", phenopacket_input.filename)
        output_path_prefix = os.path.join(args.output_dir, f"{phenopacket_input_prefix}.lirical")

        s1.output("lirical.html", output_path=f"{output_path_prefix}.html")
        s1.output("lirical.tsv", output_path=f"{output_path_prefix}.tsv")

    # run the pipeline
    bp.run()


if __name__ == "__main__":
    main()